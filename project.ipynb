{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import the Necessary Libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "SRcrzr0iNqv2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create seperate Functions for Each column to be filled\n",
        "\n",
        "def company_description(company_name,response):\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        #print(soup)\n",
        "        # Check if the 'About' section exists on the page\n",
        "        about_section = soup.find('span', {'class': 'description ng-star-inserted'})\n",
        "\n",
        "        if about_section:\n",
        "            # Check if text content is available\n",
        "            description = about_section.text.strip() if about_section.text else \"No company description available\"\n",
        "            return description\n",
        "        else:\n",
        "            return f\"No Description found for {company_name}\"\n",
        "    else:\n",
        "        return f\"Failed to retrieve information. Status code: {response.status_code}\"\n",
        "\n",
        "\n",
        "def find_sector(company_name,response):\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        #print(soup)\n",
        "        # Check if the information on sector exists on the page\n",
        "        sector = soup.find('span', {'class': 'component--field-formatter field-type-enum ng-star-inserted'})\n",
        "\n",
        "        if sector:\n",
        "            # Check if text content is available\n",
        "            description = sector.text.strip() if sector.text else \"No company description available\"\n",
        "            return description\n",
        "        else:\n",
        "            return f\"No information found for {company_name}\"\n",
        "    else:\n",
        "        return f\"Failed to retrieve information. Status code: {response.status_code}\"\n",
        "\n",
        "\n",
        "def find_industry(company_name,response):\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all div elements with the class 'chip-text'(industry)\n",
        "        chip_texts = soup.find_all('div', {'class': 'chip-text'})\n",
        "\n",
        "        if chip_texts:\n",
        "            # Extract the text content of each 'chip-text'\n",
        "            descriptions = [chip_text.text.strip() for chip_text in chip_texts]\n",
        "            industry_str = ', '.join(descriptions)\n",
        "            #print(industry_str)\n",
        "            return industry_str\n",
        "        else:\n",
        "            return f\"No Information found for {company_name}\"\n",
        "    else:\n",
        "        return f\"Failed to retrieve information. Status code: {response.status_code}\"\n",
        "\n",
        "\n",
        "def last_fund(company_name,response):\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <a> elements with the specified class\n",
        "        a_elements = soup.find_all('a', {'class': 'component--field-formatter field-type-enum accent highlight-color-contrast-light ng-star-inserted'})\n",
        "\n",
        "        if a_elements and len(a_elements) >= 2:\n",
        "            # Extract and print the text content of the second <a> element\n",
        "            description = a_elements[1].text.strip()\n",
        "            return description\n",
        "        else:\n",
        "            return f\"No Funding found for {company_name}\"\n",
        "    else:\n",
        "        return f\"Failed to retrieve information. Status code: {response.status_code}\"\n",
        "\n",
        "\n",
        "def find_competitors(company_name):\n",
        "    #since competitors information is on another page we are navigating to different api and extracting the information\n",
        "    url = f'https://www.crunchbase.com/organization/{company_name.lower()}/org_similarity_overview'\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(response.status_code)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all <a> elements with specific attributes\n",
        "        matching_a_elements = soup.find_all('div', {'class': 'reasons-container ng-star-inserted'})\n",
        "        com_name=soup.find('h1',{'class':'profile-name'})\n",
        "        if matching_a_elements and com_name:\n",
        "            # Extract and return the text content of all matching <a> elements in a list\n",
        "            results = [element.text.strip() for element in matching_a_elements]\n",
        "            comp_name=com_name.text.strip()\n",
        "            res=[]\n",
        "            #print(results)\n",
        "            for text in results:\n",
        "              # matches = re.findall(fr\"{company_name} and (\\w+)\", text)\n",
        "              pattern = re.compile(comp_name + r'\\s+and\\s+(\\w+)')\n",
        "              matches = pattern.findall(text)\n",
        "              res.extend(matches)\n",
        "            competitors_str = ', '.join(res)\n",
        "            #print(competitors_str)\n",
        "            return competitors_str\n",
        "        else:\n",
        "            return [f\"No Competitors found for {company_name}\"]\n",
        "    else:\n",
        "        return [f\"Failed to retrieve information. Status code: {response.status_code}\"]\n",
        "\n",
        "\n",
        "\n",
        "def products_services(company_name,response):\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        #print(soup)\n",
        "        # Check if the product and service information exists\n",
        "        p_s = soup.find('span', {'class': 'description has-overflow ng-star-inserted'})\n",
        "\n",
        "        if p_s:\n",
        "            # Check if text content is available\n",
        "            description = p_s.text.strip() if p_s.text else \"No company description available\"\n",
        "            return description\n",
        "        else:\n",
        "            return f\"No Products/Services found for {company_name}\"\n",
        "    else:\n",
        "        return f\"Failed to retrieve information. Status code: {response.status_code}\"\n"
      ],
      "metadata": {
        "id": "NN2Yf5f9N3MP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(name):\n",
        "  #Processing the Name of company before we pass it to the url\n",
        "  temp_name=name.lower().replace(' ', '-')\n",
        "  url = f'https://www.crunchbase.com/organization/{temp_name}'\n",
        "\n",
        "  headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "  }\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "  print(response.status_code)\n",
        "  #check the Response code from the website\n",
        "  if response.status_code==200:\n",
        "    inf1=company_description(name,response)\n",
        "    inf2=find_sector(name,response)\n",
        "    inf3=find_industry(name,response)\n",
        "    inf4=last_fund(name,response)\n",
        "    inf5=find_competitors(temp_name)\n",
        "    inf6=products_services(name,response)\n",
        "    # print(f\"{inf1} {inf2} {inf3} {inf4} {inf6}\")\n",
        "    # print(inf5)\n",
        "    return {\n",
        "            'Company Name' : name,\n",
        "            'Company Description': inf1,\n",
        "            'Sector': inf2,\n",
        "            'Industry': inf3,\n",
        "            'Funding': inf4,\n",
        "            'Products/Services Description': inf6,\n",
        "            'Competitors': inf5\n",
        "        }\n",
        "  else:\n",
        "      print(f\"Failed to retrieve information for {name}\")\n",
        "      return None\n"
      ],
      "metadata": {
        "id": "srYrgq67kakJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Excel File for List of Companies\n",
        "def generate_excel_file(company_names):\n",
        "    data = []\n",
        "    #Processing Each Company\n",
        "    for company_name in company_names:\n",
        "        company_info = test(company_name)\n",
        "        time.sleep(4) #Introducing Delay so not to spam requests\n",
        "        if company_info:\n",
        "            data.append(company_info)\n",
        "\n",
        "    #concverting the Information into a Data Frame\n",
        "    df = pd.DataFrame(data)\n",
        "    #converting The Dataframe to Excel File\n",
        "    excel_filename = 'output_file.xlsx'\n",
        "\n",
        "    # Please Note that if You are Ruinning the Code on a local machine make sure that you create an Empty excel file in your Working Directory,\n",
        "    # copy the file path and provide it to the code\n",
        "    # #example\n",
        "    # excel_filename='r'C:\\Users\\Vara\\OneDrive\\Desktop\\Karya\\ouput_file.xlsx'\n",
        "    # Do not forget to add your correct file path'''\n",
        "\n",
        "    df.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f'Excel file \"{excel_filename}\" generated successfully.')\n",
        "\n",
        "if __name__=='__main__':\n",
        "  # Example usage\n",
        "  company_names = ['Amazon','Flipkart','Google', 'ZS Associates']#['CompanyA', 'CompanyB', 'CompanyC']\n",
        "  generate_excel_file(company_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixll2eF6uf3J",
        "outputId": "6265c71c-1589-4fcf-9f3b-115e0eec5b0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "Excel file \"output_file.xlsx\" generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "982NZNrSJR91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}